{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "965f0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textstat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0437782a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../data/train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2982798d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2834, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54774883",
   "metadata": {},
   "source": [
    "# Phase 1: Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20101cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistical_features(text):\n",
    "    # a. basic count\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = textstat.sentence_count(text)\n",
    "\n",
    "    # b. length statistics\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    avg_word_length = sum(len(word) for word in text.split()\n",
    "                          ) / word_count if word_count > 0 else 0\n",
    "\n",
    "    # c. textstat\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "\n",
    "    # d. lexical variation\n",
    "    lexical_diversity = textstat.lexicon_count(\n",
    "        text, removepunct=True) / word_count if word_count > 0 else 0\n",
    "\n",
    "    return [\n",
    "        word_count, sentence_count, avg_sentence_length, avg_word_length,\n",
    "        flesch_reading_ease, flesch_kincaid_grade, gunning_fog, lexical_diversity\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8b2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>lexical_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>11</td>\n",
       "      <td>16.272727</td>\n",
       "      <td>4.547486</td>\n",
       "      <td>79.251143</td>\n",
       "      <td>6.247984</td>\n",
       "      <td>8.743728</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169</td>\n",
       "      <td>14</td>\n",
       "      <td>12.071429</td>\n",
       "      <td>4.550296</td>\n",
       "      <td>78.945814</td>\n",
       "      <td>5.246851</td>\n",
       "      <td>6.958749</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166</td>\n",
       "      <td>12</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>4.475904</td>\n",
       "      <td>78.125492</td>\n",
       "      <td>5.798976</td>\n",
       "      <td>7.702008</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164</td>\n",
       "      <td>5</td>\n",
       "      <td>32.800000</td>\n",
       "      <td>4.548780</td>\n",
       "      <td>70.372268</td>\n",
       "      <td>11.592244</td>\n",
       "      <td>13.607805</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>3.925170</td>\n",
       "      <td>79.157265</td>\n",
       "      <td>9.522259</td>\n",
       "      <td>12.848435</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  sentence_count  avg_sentence_length  avg_word_length  \\\n",
       "0         179              11            16.272727         4.547486   \n",
       "1         169              14            12.071429         4.550296   \n",
       "2         166              12            13.833333         4.475904   \n",
       "3         164               5            32.800000         4.548780   \n",
       "4         147               5            29.400000         3.925170   \n",
       "\n",
       "   flesch_reading_ease  flesch_kincaid_grade  gunning_fog  lexical_diversity  \n",
       "0            79.251143              6.247984     8.743728                1.0  \n",
       "1            78.945814              5.246851     6.958749                1.0  \n",
       "2            78.125492              5.798976     7.702008                1.0  \n",
       "3            70.372268             11.592244    13.607805                1.0  \n",
       "4            79.157265              9.522259    12.848435                1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [\n",
    "    'word_count', 'sentence_count', 'avg_sentence_length', 'avg_word_length',\n",
    "    'flesch_reading_ease', 'flesch_kincaid_grade', 'gunning_fog', 'lexical_diversity'\n",
    "]\n",
    "\n",
    "features_df = pd.DataFrame(df['excerpt'].apply(\n",
    "    get_statistical_features).tolist(), columns=feature_names)\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f905a",
   "metadata": {},
   "source": [
    "## baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51289475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2267, 8)\n",
      "(567, 8)\n"
     ]
    }
   ],
   "source": [
    "X = features_df\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae4bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- baseline model evaluation ---\n",
      "RMSE: 0.8884\n"
     ]
    }
   ],
   "source": [
    "baseline_model = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=42, n_jobs=-1)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = baseline_model.predict(X_test)\n",
    "\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f\"\\n--- baseline model evaluation ---\")\n",
    "print(f\"RMSE: {baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d0ef5",
   "metadata": {},
   "source": [
    "# Phase 2: Add TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91585417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2267, 6)\n",
      "(567, 6)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = train_df['target']\n",
    "y_test = test_df['target']\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009dfd3e",
   "metadata": {},
   "source": [
    "## statistical feature from phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stats = pd.DataFrame(train_df['excerpt'].apply(\n",
    "    get_statistical_features).tolist(), columns=feature_names)\n",
    "X_test_stats = pd.DataFrame(test_df['excerpt'].apply(\n",
    "    get_statistical_features).tolist(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ff973",
   "metadata": {},
   "source": [
    "## tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000,\n",
    "                                   stop_words='english',\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "# Fit TF-IDF on the training data and then Transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['excerpt'])\n",
    "\n",
    "# Only Transform on the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['excerpt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae03e78",
   "metadata": {},
   "source": [
    "## merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e1d81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2267, 1008)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the statistical features (dense) into a sparse matrix\n",
    "X_train_stats_sparse = csr_matrix(X_train_stats)\n",
    "X_test_stats_sparse = csr_matrix(X_test_stats)\n",
    "\n",
    "# use hstack (horizontal stacking) to concatenate the features together\n",
    "X_train_combined = hstack([X_train_stats_sparse, X_train_tfidf])\n",
    "X_test_combined = hstack([X_test_stats_sparse, X_test_tfidf])\n",
    "\n",
    "X_train_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fff05a",
   "metadata": {},
   "source": [
    "## second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fa978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- model evaluation ---\n",
      "Phase 1 (Only Statistical Features) RMSE: 0.8884\n",
      "Phase 2 (Statistics + TF-IDF) RMSE: 0.7891\n"
     ]
    }
   ],
   "source": [
    "model_phase2 = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_phase2.fit(X_train_combined, y_train)\n",
    "\n",
    "predictions_phase2 = model_phase2.predict(X_test_combined)\n",
    "rmse_phase2 = np.sqrt(mean_squared_error(y_test, predictions_phase2))\n",
    "\n",
    "print(\"\\n--- model evaluation ---\")\n",
    "print(f\"Phase 1 (Only Statistical Features) RMSE: {baseline_rmse:.4f}\")\n",
    "print(f\"Phase 2 (Statistics + TF-IDF) RMSE: {rmse_phase2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67359e3",
   "metadata": {},
   "source": [
    "# Phase 3: Add Bigram Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc899a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Gensim 'glove-wiki-gigaword-100' model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# download a pre-trained model\n",
    "wv_model = api.load('glove-wiki-gigaword-100')\n",
    "print(\"Gensim 'glove-wiki-gigaword-100' model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4581c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import NLTK for word segmentation\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61adfe8",
   "metadata": {},
   "source": [
    "## train the Bigram detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63160b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing training text for bigram detection\n",
    "tokenized_train_text = [word_tokenize(text.lower())\n",
    "                        for text in train_df['excerpt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604aab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Bigram (Phrases) model\n",
    "bigram_model = gensim.models.Phrases(\n",
    "    tokenized_train_text, min_count=5, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a94fcee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Phraser model\n",
    "bigram_phraser = gensim.models.phrases.Phraser(bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5793948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['When', 'the', 'young', 'people', 'returned', 'to', 'the', 'ballroom,', 'it', 'presented', 'a', 'decidedly', 'changed', 'appearance.', 'Instead', 'of', 'an', 'interior', 'scene,', 'it', 'was', 'a', 'winter', 'landscape.', 'The', 'floor', 'was', 'covered', 'with', 'snow-white', 'canvas,', 'not', 'laid', 'on', 'smoothly,', 'but', 'rumpled', 'over', 'bumps', 'and', 'hillocks,', 'like', 'a', 'real', 'snow', 'field.', 'The', 'numerous', 'palms', 'and', 'evergreens', 'that', 'had', 'decorated', 'the', 'room,', 'were', 'powdered', 'with', 'flour', 'and', 'strewn', 'with', 'tufts', 'of', 'cotton,', 'like', 'snow.', 'Also', 'diamond', 'dust', 'had', 'been', 'lightly', 'sprinkled', 'on', 'them,', 'and', 'glittering', 'crystal', 'icicles', 'hung', 'from', 'the', 'branches.', 'At', 'each', 'end', 'of', 'the', 'room,', 'on', 'the', 'wall,', 'hung', 'a', 'beautiful', 'bear-skin', 'rug.', 'These', 'rugs', 'were', 'for', 'prizes,', 'one', 'for', 'the', 'girls', 'and', 'one', 'for', 'the', 'boys.', 'And', 'this', 'was', 'the', 'game.', 'The', 'girls', 'were', 'gathered', 'at', 'one', 'end', 'of', 'the', 'room', 'and', 'the', 'boys', 'at', 'the', 'other,', 'and', 'one', 'end', 'was', 'called', 'the', 'North', 'Pole,', 'and', 'the', 'other', 'the', 'South', 'Pole.', 'Each', 'player', 'was', 'given', 'a', 'small', 'flag', 'which', 'they', 'were', 'to', 'plant', 'on', 'reaching', 'the', 'Pole.', 'This', 'would', 'have', 'been', 'an', 'easy', 'matter,', 'but', 'each', 'traveller', 'was', 'obliged', 'to', 'wear', 'snowshoes.']\n",
      "With Bigrams: ['When', 'the', 'young_people', 'returned', 'to', 'the', 'ballroom,', 'it', 'presented', 'a', 'decidedly', 'changed', 'appearance.', 'Instead', 'of', 'an', 'interior', 'scene,', 'it', 'was', 'a', 'winter', 'landscape.', 'The', 'floor', 'was', 'covered_with', 'snow-white', 'canvas,', 'not', 'laid', 'on', 'smoothly,', 'but', 'rumpled', 'over', 'bumps', 'and', 'hillocks,', 'like', 'a', 'real', 'snow', 'field.', 'The', 'numerous', 'palms', 'and', 'evergreens', 'that', 'had', 'decorated', 'the', 'room,', 'were', 'powdered', 'with', 'flour', 'and', 'strewn', 'with', 'tufts', 'of', 'cotton,', 'like', 'snow.', 'Also', 'diamond', 'dust', 'had_been', 'lightly', 'sprinkled', 'on', 'them,', 'and', 'glittering', 'crystal', 'icicles', 'hung', 'from', 'the', 'branches.', 'At', 'each', 'end', 'of', 'the', 'room,', 'on', 'the', 'wall,', 'hung', 'a', 'beautiful', 'bear-skin', 'rug.', 'These', 'rugs', 'were', 'for', 'prizes,', 'one', 'for', 'the', 'girls', 'and', 'one', 'for', 'the', 'boys.', 'And', 'this', 'was', 'the', 'game.', 'The', 'girls', 'were', 'gathered', 'at', 'one', 'end', 'of', 'the', 'room', 'and', 'the', 'boys', 'at', 'the', 'other,', 'and', 'one', 'end', 'was', 'called', 'the', 'North', 'Pole,', 'and', 'the', 'other', 'the', 'South', 'Pole.', 'Each', 'player', 'was', 'given', 'a', 'small', 'flag', 'which', 'they_were', 'to', 'plant', 'on', 'reaching', 'the', 'Pole.', 'This', 'would_have', 'been', 'an', 'easy', 'matter,', 'but', 'each', 'traveller', 'was', 'obliged_to', 'wear', 'snowshoes.']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape. The floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches. At each end of the room, on the wall, hung a beautiful bear-skin rug. These rugs were for prizes, one for the girls and one for the boys. And this was the game. The girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole. This would have been an easy matter, but each traveller was obliged to wear snowshoes.\".split()\n",
    "print(f\"Original: {test_sentence}\")\n",
    "print(f\"With Bigrams: {bigram_phraser[test_sentence]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55638f05",
   "metadata": {},
   "source": [
    "## document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(text, wv_model, phraser, vector_dim=100):\n",
    "    # tokenize and apply bigram phrase generator\n",
    "    tokens = phraser[word_tokenize(text.lower())]\n",
    "\n",
    "    # obtain the vector for each word\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in wv_model:\n",
    "            vectors.append(wv_model[word])\n",
    "\n",
    "    if not vectors:\n",
    "        return np.zeros(vector_dim)\n",
    "\n",
    "    # aggregation: calculate the average of all word vectors\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2267, 100)\n",
      "(567, 100)\n"
     ]
    }
   ],
   "source": [
    "# creating document vectors for train and test sets\n",
    "X_train_w2v = np.array([get_document_vector(\n",
    "    text, wv_model, bigram_phraser) for text in train_df['excerpt']])\n",
    "X_test_w2v = np.array([get_document_vector(\n",
    "    text, wv_model, bigram_phraser) for text in test_df['excerpt']])\n",
    "\n",
    "print(X_train_w2v.shape)\n",
    "print(X_test_w2v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d9c73",
   "metadata": {},
   "source": [
    "## merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00660cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2267, 1108)\n",
      "(567, 1108)\n"
     ]
    }
   ],
   "source": [
    "# convert the new W2V features (dense) into a sparse matrix\n",
    "X_train_w2v_sparse = csr_matrix(X_train_w2v)\n",
    "X_test_w2v_sparse = csr_matrix(X_test_w2v)\n",
    "\n",
    "X_train_final = hstack([X_train_combined, X_train_w2v_sparse])\n",
    "X_test_final = hstack([X_test_combined, X_test_w2v_sparse])\n",
    "\n",
    "print(X_train_final.shape)\n",
    "print(X_test_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fa458",
   "metadata": {},
   "source": [
    "## third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02786f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- model evaluation ---\n",
      "Phase 1 (Only Statistical Features) RMSE: 0.8884\n",
      "Phase 2 (Statistics + TF-IDF) RMSE: 0.7891\n",
      "Phase 3 (Stats + TF-IDF + Bigram W2V) RMSE: 0.6997\n"
     ]
    }
   ],
   "source": [
    "model_phase3 = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_phase3.fit(X_train_final, y_train)\n",
    "\n",
    "predictions_phase3 = model_phase3.predict(X_test_final)\n",
    "rmse_phase3 = np.sqrt(mean_squared_error(y_test, predictions_phase3))\n",
    "\n",
    "print(\"\\n--- model evaluation ---\")\n",
    "print(f\"Phase 1 (Only Statistical Features) RMSE: {baseline_rmse:.4f}\")\n",
    "print(f\"Phase 2 (Statistics + TF-IDF) RMSE: {rmse_phase2:.4f}\")\n",
    "print(f\"Phase 3 (Stats + TF-IDF + Bigram W2V) RMSE: {rmse_phase3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2aa09",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12abe55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The 5 worst examples (where the model made the most errors) ---\n",
      "             id                                          url_legal    license  \\\n",
      "1637  8662922c7                                                NaN        NaN   \n",
      "1642  ee7d40251                                                NaN        NaN   \n",
      "2226  513bd77b3                                                NaN        NaN   \n",
      "1770  968c283cd                                                NaN        NaN   \n",
      "2731  76fe1a630  https://freekidsbooks.org/wp-content/uploads/2...  CC BY 4.0   \n",
      "\n",
      "                                                excerpt    target  \\\n",
      "1637  A first-class boat will be of about the follow... -3.236543   \n",
      "1642  We have frequent inquiries as to the best mean... -3.585369   \n",
      "2226  The Goban was the master of sixteen trades. Th... -1.819263   \n",
      "1770  \"The principal generators of incrustation in b... -3.373600   \n",
      "2731  Helen Adams Keller was born on 27th June 1880,...  0.646549   \n",
      "\n",
      "      standard_error  predicted_target     error  abs_error  \n",
      "1637        0.582070         -1.221065  2.015478   2.015478  \n",
      "1642        0.588952         -1.571092  2.014277   2.014277  \n",
      "2226        0.494852          0.089855  1.909118   1.909118  \n",
      "1770        0.617688         -1.581489  1.792111   1.792111  \n",
      "2731        0.516289         -1.126084 -1.772633   1.772633  \n",
      "\n",
      "--- The top 5 best-performing examples (with the most accurate models) ---\n",
      "             id                                          url_legal  \\\n",
      "679   10112f396  https://kids.frontiersin.org/article/10.3389/f...   \n",
      "2448  a20b7c831                                                NaN   \n",
      "741   33030f528       https://simple.wikipedia.org/wiki/Seismology   \n",
      "798   1c66b6ca4  https://kids.frontiersin.org/article/10.3389/f...   \n",
      "2531  29784afb6                                                NaN   \n",
      "\n",
      "                    license  \\\n",
      "679               CC BY 4.0   \n",
      "2448                    NaN   \n",
      "741   CC BY-SA 3.0 and GFDL   \n",
      "798               CC BY 4.0   \n",
      "2531                    NaN   \n",
      "\n",
      "                                                excerpt    target  \\\n",
      "679   Before we dive into talking about brain injury... -0.169105   \n",
      "2448  Mr. Graham always spoke of his wife's dressing... -0.848790   \n",
      "741   Seismology is the study of what is under the s... -1.154857   \n",
      "798   An antibiotic is a compound that kills bacteri... -0.647144   \n",
      "2531  The golden days of October passed away, as so ... -0.742836   \n",
      "\n",
      "      standard_error  predicted_target     error  abs_error  \n",
      "679         0.489799         -0.170176 -0.001071   0.001071  \n",
      "2448        0.469961         -0.847034  0.001756   0.001756  \n",
      "741         0.479173         -1.151876  0.002981   0.002981  \n",
      "798         0.457757         -0.641114  0.006029   0.006029  \n",
      "2531        0.460632         -0.736674  0.006162   0.006162  \n"
     ]
    }
   ],
   "source": [
    "results_df = test_df.copy()\n",
    "results_df['predicted_target'] = predictions_phase3\n",
    "results_df['error'] = results_df['predicted_target'] - results_df['target']\n",
    "results_df['abs_error'] = np.abs(results_df['error'])\n",
    "\n",
    "# identify the example with the poorest performance (the largest error)\n",
    "print(\"--- The 5 worst examples (where the model made the most errors) ---\")\n",
    "print(results_df.sort_values(by='abs_error', ascending=False).head(5))\n",
    "\n",
    "# Identify the best example (the one with the smallest error)\n",
    "print(\"\\n--- The top 5 best-performing examples (with the most accurate models) ---\")\n",
    "print(results_df.sort_values(by='abs_error', ascending=True).head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
